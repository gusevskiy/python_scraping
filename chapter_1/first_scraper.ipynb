{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "try:\n",
    "    html = urlopen('https://pythonscrapingthisurldoesnotexist.com')\n",
    "except HTTPError as e:\n",
    "    print(e)\n",
    "except URLError as e:\n",
    "    print('The server could not be found!')\n",
    "else:\n",
    "    print('It Worked!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "\n",
    "try:\n",
    "    html = urlopen(\"https://pythonscrapingthisurldoesnotexist.com\")\n",
    "except HTTPError as e:\n",
    "    print(\"The server returned an HTTP error\")\n",
    "except URLError as e:\n",
    "    print(\"The server could not be found!\")\n",
    "else:\n",
    "    print(html.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def getTitle(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    try:\n",
    "        bsObj = BeautifulSoup(html.read(), \"lxml\")\n",
    "        title = bsObj.body.h1\n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "    return title\n",
    "\n",
    "\n",
    "title = getTitle(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "if title is None:\n",
    "    print(\"Title could not be found\")\n",
    "else:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('http://www.pythonscraping.com/pages/warandpeace.html')\n",
    "bs = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna\n",
      "Pavlovna Scherer\n",
      "Empress Marya\n",
      "Fedorovna\n",
      "Prince Vasili Kuragin\n",
      "Anna Pavlovna\n",
      "St. Petersburg\n",
      "the prince\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "the prince\n",
      "the prince\n",
      "Prince Vasili\n",
      "Anna Pavlovna\n",
      "Anna Pavlovna\n",
      "the prince\n",
      "Wintzingerode\n",
      "King of Prussia\n",
      "le Vicomte de Mortemart\n",
      "Montmorencys\n",
      "Rohans\n",
      "Abbe Morio\n",
      "the Emperor\n",
      "the prince\n",
      "Prince Vasili\n",
      "Dowager Empress Marya Fedorovna\n",
      "the baron\n",
      "Anna Pavlovna\n",
      "the Empress\n",
      "the Empress\n",
      "Anna Pavlovna's\n",
      "Her Majesty\n",
      "Baron\n",
      "Funke\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "the Empress\n",
      "The prince\n",
      "Anatole\n",
      "the prince\n",
      "The prince\n",
      "Anna\n",
      "Pavlovna\n",
      "Anna Pavlovna\n"
     ]
    }
   ],
   "source": [
    "nameList = bs.findAll('span', {'class': 'green'})\n",
    "for name in nameList:\n",
    "    print(name.get_text())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
